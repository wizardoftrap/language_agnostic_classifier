{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Q1u9ynfFuR_"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets scikit-learn tqdm sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q33G2ffMGHuv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6fO-KP1GL3-"
   },
   "outputs": [],
   "source": [
    "class LanguageAgnosticClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Linear(self.hidden, num_labels)\n",
    "\n",
    "    def mean_pool(self, hidden, mask):\n",
    "        mask = mask.unsqueeze(-1).float()\n",
    "        return (hidden * mask).sum(1) / mask.sum(1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self.mean_pool(out.last_hidden_state, attention_mask)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits, pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMHM4kg2GQu_"
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super().__init__()\n",
    "        self.temp = temperature\n",
    "\n",
    "    def forward(self, z1, z2):\n",
    "        z1 = F.normalize(z1, dim=1)\n",
    "        z2 = F.normalize(z2, dim=1)\n",
    "        sim = torch.matmul(z1, z2.T) / self.temp\n",
    "        labels = torch.arange(z1.size(0)).to(z1.device)\n",
    "        return F.cross_entropy(sim, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDGHJ7YYGT0_"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class EnglishDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split=\"train\"):\n",
    "        self.data = load_dataset(\"imdb\", split=split)\n",
    "\n",
    "    def augment(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) > 6:\n",
    "            random.shuffle(words)\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"text\"]\n",
    "        label = self.data[idx][\"label\"]\n",
    "\n",
    "        aug = self.augment(text)\n",
    "\n",
    "        enc1 = tokenizer(text, truncation=True, padding=\"max_length\",\n",
    "                          max_length=128, return_tensors=\"pt\")\n",
    "        enc2 = tokenizer(aug, truncation=True, padding=\"max_length\",\n",
    "                          max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"ids1\": enc1[\"input_ids\"].squeeze(),\n",
    "            \"mask1\": enc1[\"attention_mask\"].squeeze(),\n",
    "            \"ids2\": enc2[\"input_ids\"].squeeze(),\n",
    "            \"mask2\": enc2[\"attention_mask\"].squeeze(),\n",
    "            \"label\": torch.tensor(label)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "tgiKZMIBGdE3"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_ds = EnglishDataset(\"train\")\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-An-2RiGsHP"
   },
   "outputs": [],
   "source": [
    "model = LanguageAgnosticClassifier(MODEL_NAME, num_labels=2).to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "ctr_loss = ContrastiveLoss()\n",
    "\n",
    "LAMBDA = 0.5\n",
    "EPOCHS = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Xnj5d4dG0SH"
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids1 = batch[\"ids1\"].to(DEVICE)\n",
    "        mask1 = batch[\"mask1\"].to(DEVICE)\n",
    "        ids2 = batch[\"ids2\"].to(DEVICE)\n",
    "        mask2 = batch[\"mask2\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        logits, emb1 = model(ids1, mask1)\n",
    "        _, emb2 = model(ids2, mask2)\n",
    "\n",
    "        loss_cls = ce_loss(logits, labels)\n",
    "        loss_ctr = ctr_loss(emb1, emb2)\n",
    "\n",
    "        loss = loss_cls + LAMBDA * loss_ctr\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (logits.argmax(1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss {total_loss/len(train_loader):.4f} | Acc {correct/total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhVIzAI3G46q"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"english_trained_multilingual.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkmTTTU3G9IL"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "def predict(text):\n",
    "    enc = tokenizer(text, return_tensors=\"pt\",\n",
    "                    truncation=True, padding=\"max_length\", max_length=128)\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(enc[\"input_ids\"].to(DEVICE),\n",
    "                          enc[\"attention_mask\"].to(DEVICE))\n",
    "    return \"POSITIVE\" if logits.argmax(1).item() == 1 else \"NEGATIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srMfu9wDHChb"
   },
   "outputs": [],
   "source": [
    "print(\"English:\", predict(\"This movie was absolutely amazing\"))\n",
    "print(\"Hindi:\", predict(\"यह फिल्म बहुत शानदार थी\"))\n",
    "print(\"Hindi:\", predict(\"यह फिल्म बहुत खराब थी\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F20hLujLQ181"
   },
   "outputs": [],
   "source": [
    "tests_en = [\n",
    "    \"This movie was absolutely fantastic\",\n",
    "    \"I really enjoyed the story and acting\",\n",
    "    \"The film was boring and a complete waste of time\",\n",
    "    \"Terrible movie, I regret watching it\",\n",
    "    \"An excellent performance by the lead actor\",\n",
    "    \"The plot was weak and predictable\",\n",
    "]\n",
    "tests_hi = [\n",
    "    \"यह फिल्म बहुत शानदार थी\",\n",
    "    \"मुझे यह फिल्म बहुत पसंद आई\",\n",
    "    \"यह फिल्म बहुत खराब थी\",\n",
    "    \"कहानी बिल्कुल बेकार थी\",\n",
    "    \"अभिनय शानदार था\",\n",
    "    \"यह समय की पूरी बर्बादी थी\",\n",
    "]\n",
    "tests_hi_long = [\n",
    "    \"यह फिल्म देखने लायक है और कहानी भी अच्छी है\",\n",
    "    \"फिल्म की कहानी कमजोर थी लेकिन अभिनय अच्छा था\",\n",
    "    \"मुझे यह फिल्म बिल्कुल पसंद नहीं आई\",\n",
    "    \"फिल्म बहुत लंबी और उबाऊ लग रही थी\",\n",
    "]\n",
    "\n",
    "tests_code_mixed = [\n",
    "    \"यह movie बहुत अच्छी थी\",\n",
    "    \"Story अच्छी थी but execution खराब था\",\n",
    "    \"Acting तो अच्छी थी लेकिन movie boring थी\",\n",
    "    \"यह फिल्म totally waste of time थी\",\n",
    "]\n",
    "tests_tricky = [\n",
    "    \"The movie was not bad\",\n",
    "    \"यह फिल्म बुरी नहीं थी\",\n",
    "    \"I expected more from this movie\",\n",
    "    \"फिल्म ठीक-ठाक थी\",\n",
    "    \"The movie was average at best\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0FMxz83RIN1"
   },
   "outputs": [],
   "source": [
    "all_tests = (\n",
    "    tests_en\n",
    "    + tests_hi\n",
    "    + tests_hi_long\n",
    "    + tests_code_mixed\n",
    "    + tests_tricky\n",
    ")\n",
    "\n",
    "for text in all_tests:\n",
    "    print(f\"{text} --> {predict(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "id7okVMGSfBM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SAVE_DIR = \"/content/drive/MyDrive/language_agnostic_classifier\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "SAVE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CSKsAfASiRW"
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = f\"{SAVE_DIR}/english_trained_multilingual.pt\"\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "MODEL_PATH"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
